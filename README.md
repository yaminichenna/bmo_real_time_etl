# ğŸ’µ BMO Real-Time ETL Pipeline & Dashboard (AWS + Python)
![Python](https://img.shields.io/badge/Python-3.12-blue)
![AWS](https://img.shields.io/badge/Hosted%20on-AWS-FF9900)
![ETL](https://img.shields.io/badge/ETL%20Pipeline-Redshift%20%7C%20S3%20%7C%20Postgres-green)
![Dashboard](https://img.shields.io/badge/Dashboard-Streamlit-blueviolet)

A full ETL project that extracts real-time cash flow data for **Bank of Montreal (BMO)** using public APIs, transforms it with Python, stores it in AWS Redshift, and visualizes it with a live dashboard built in Streamlit and hosted on AWS EC2.

---


## ğŸ“Œ Project Overview

- ğŸ”„ **Extract**: BMO cash flow data from Financial Modeling Prep API
- ğŸ§¹ **Transform**: Clean & format using Pandas
- ğŸ’¾ **Load**: Into PostgreSQL â†’ AWS S3 â†’ Redshift
- ğŸ“Š **Dashboard**: Streamlit app hosted on EC2

---

## ğŸ§± Architecture

[Financial Modeling Prep API]
           â†“
    ğŸ Python Extract Script
           â†“
      ğŸ“„ Raw CSV File
           â†“
  ğŸ” Transform with Pandas
           â†“
      ğŸ“„ Cleaned CSV File
           â†“
 PostgreSQL (local staging DB)
           â†“
          â¬‡
 Upload raw & cleaned files â†’ ğŸª£ AWS S3
           â†“
      COPY to Redshift Serverless
           â†“
 ğŸ” Streamlit Dashboard (on EC2)

![ETL Architecture](architecture.png)

---


## âš™ï¸ Tech Stack

- **Python 3.12**
- **Pandas**, **psycopg2**, **boto3**
- **PostgreSQL** (local staging DB)
- **AWS S3** (cloud file storage)
- **Amazon Redshift Serverless** (data warehouse)
- **Streamlit** (dashboard UI)
- **AWS EC2** (dashboard hosting)
- **Git & GitHub** (version control)
---

## ğŸš€ How to Run This Project

```bash
git clone https://github.com/yaminichenna/bmo_real_time_etl.git
cd bmo_real_time_etl
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```



## ğŸ”„ Run the ETL Pipeline

Follow these steps to extract, clean, load, and store BMO cash flow data:
```bash
# Step 1: Extract raw BMO cash flow data from API
python extract/fetch_bmo_cashflow.py

# Step 2: Transform the data using pandas
python transform/transform_cashflow.py

# Step 3: Load into local PostgreSQL staging database
python load/load_to_postgres.py

# Step 4: Upload both raw and cleaned files to S3
python cloud/upload_to_s3.py

```


## ğŸ“· Dashboard Preview

![Dashboard Screenshot](dashboard/demo_screenshot.png)

## ğŸ“ Folder Structure
```bash
bmo_real_time_etl/
â”œâ”€â”€ extract/              # Fetches API data
â”œâ”€â”€ transform/            # Cleans/transforms data
â”œâ”€â”€ load/                 # Loads into PostgreSQL
â”œâ”€â”€ cloud/                # Uploads to S3
â”œâ”€â”€ dashboard/            # Streamlit dashboard
â”œâ”€â”€ architecture.png      # Visual representation of the flow
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```


## ğŸ§  What You'll Learn

- **Building end-to-end ETL with real-time data**

- **Extracting public financial data via REST APIs**

- **Loading to PostgreSQL, S3, and Redshift**

- **Using IAM roles and secure data access**

- **Creating dashboards with Streamlit**

- **Hosting apps on AWS EC2**

## ğŸ”® Future Enhancements

- **â° Add Airflow for automation & orchestration**

- **ğŸ³ Add Docker support**

- **ğŸ“¡ Add monitoring/logging with CloudWatch**

- **ğŸ“ˆ Add more KPIs & anomaly detection**

- **ğŸ“¤ Email reports or auto-alerts**

## ğŸ‘¨â€ğŸ’» Author
**Yamini Chenna**  
ğŸ”— [GitHub Profile](https://github.com/yaminichenna/bmo_real_time_etl)


